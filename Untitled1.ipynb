{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4953,  0.4743,  0.4356,  0.0763,  0.6213,  0.9812,  0.6306,\n",
       "           0.4136,  0.5519,  0.0650],\n",
       "         [ 0.9735,  0.5310,  0.4418,  0.6497,  0.5310,  0.8735,  0.0533,\n",
       "           0.1252,  0.4553,  0.1017],\n",
       "         [ 0.8580,  0.8117,  0.2308,  0.8561,  0.3547,  0.0198,  0.2438,\n",
       "           0.8340,  0.4619,  0.3087],\n",
       "         [ 0.6935,  0.7737,  0.2532,  0.8362,  0.8417,  0.5972,  0.3670,\n",
       "           0.8666,  0.1458,  0.8510],\n",
       "         [ 0.1908,  0.9060,  0.1180,  0.5988,  0.8177,  0.7086,  0.4357,\n",
       "           0.0883,  0.2046,  0.0971]],\n",
       "\n",
       "        [[ 0.5473,  0.1385,  0.5631,  0.2907,  0.2083,  0.8503,  0.8611,\n",
       "           0.1238,  0.1216,  0.4797],\n",
       "         [ 0.7353,  0.7494,  0.3298,  0.8544,  0.6060,  0.9084,  0.0705,\n",
       "           0.3274,  0.5405,  0.7399],\n",
       "         [ 0.9508,  0.4088,  0.4865,  0.6346,  0.2472,  0.7347,  0.3464,\n",
       "           0.3470,  0.0306,  0.1726],\n",
       "         [ 0.5202,  0.4331,  0.9309,  0.6747,  0.5151,  0.9244,  0.9707,\n",
       "           0.2755,  0.1197,  0.4287],\n",
       "         [ 0.1605,  0.6507,  0.7693,  0.9731,  0.6791,  0.0371,  0.2126,\n",
       "           0.4774,  0.4011,  0.0673]],\n",
       "\n",
       "        [[ 0.8176,  0.9442,  0.8203,  0.9278,  0.3013,  0.3588,  0.9899,\n",
       "           0.4079,  0.0839,  0.5525],\n",
       "         [ 0.9764,  0.4313,  0.8634,  0.7673,  0.6412,  0.2087,  0.2528,\n",
       "           0.2554,  0.9294,  0.3896],\n",
       "         [ 0.0869,  0.0412,  0.7398,  0.7238,  0.9260,  0.6568,  0.7979,\n",
       "           0.0049,  0.1895,  0.7760],\n",
       "         [ 0.3781,  0.8504,  0.9529,  0.6834,  0.1466,  0.1152,  0.4473,\n",
       "           0.2206,  0.1046,  0.4812],\n",
       "         [ 0.7438,  0.7733,  0.8906,  0.4094,  0.6610,  0.0023,  0.4233,\n",
       "           0.5740,  0.8585,  0.0488]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=torch.rand(3,5,10)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4953,  0.4743,  0.4356,  0.0763,  0.6213,  0.9812,  0.6306,\n",
       "           0.4136,  0.5519,  0.0650],\n",
       "         [ 0.9735,  0.5310,  0.4418,  0.6497,  0.5310,  0.8735,  0.0533,\n",
       "           0.1252,  0.4553,  0.1017],\n",
       "         [ 0.8580,  0.8117,  0.2308,  0.8561,  0.3547,  0.0198,  0.2438,\n",
       "           0.8340,  0.4619,  0.3087],\n",
       "         [ 0.6935,  0.7737,  0.2532,  0.8362,  0.8417,  0.5972,  0.3670,\n",
       "           0.8666,  0.1458,  0.8510],\n",
       "         [ 0.1908,  0.9060,  0.1180,  0.5988,  0.8177,  0.7086,  0.4357,\n",
       "           0.0883,  0.2046,  0.0971]],\n",
       "\n",
       "        [[ 0.5473,  0.1385,  0.5631,  0.2907,  0.2083,  0.8503,  0.8611,\n",
       "           0.1238,  0.1216,  0.4797],\n",
       "         [ 0.7353,  0.7494,  0.3298,  0.8544,  0.6060,  0.9084,  0.0705,\n",
       "           0.3274,  0.5405,  0.7399],\n",
       "         [ 0.9508,  0.4088,  0.4865,  0.6346,  0.2472,  0.7347,  0.3464,\n",
       "           0.3470,  0.0306,  0.1726],\n",
       "         [ 0.5202,  0.4331,  0.9309,  0.6747,  0.5151,  0.9244,  0.9707,\n",
       "           0.2755,  0.1197,  0.4287],\n",
       "         [ 0.1605,  0.6507,  0.7693,  0.9731,  0.6791,  0.0371,  0.2126,\n",
       "           0.4774,  0.4011,  0.0673]],\n",
       "\n",
       "        [[ 0.8176,  0.9442,  0.8203,  0.9278,  0.3013,  0.3588,  0.9899,\n",
       "           0.4079,  0.0839,  0.5525],\n",
       "         [ 0.9764,  0.4313,  0.8634,  0.7673,  0.6412,  0.2087,  0.2528,\n",
       "           0.2554,  0.9294,  0.3896],\n",
       "         [ 0.0869,  0.0412,  0.7398,  0.7238,  0.9260,  0.6568,  0.7979,\n",
       "           0.0049,  0.1895,  0.7760],\n",
       "         [ 0.3781,  0.8504,  0.9529,  0.6834,  0.1466,  0.1152,  0.4473,\n",
       "           0.2206,  0.1046,  0.4812],\n",
       "         [ 0.7438,  0.7733,  0.8906,  0.4094,  0.6610,  0.0023,  0.4233,\n",
       "           0.5740,  0.8585,  0.0488]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.masked_fill(torch.ByteTensor(np.array([0]*5).reshape(-1,1)),-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [ 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ByteTensor(np.array([1]*5).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d['a']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get('b',888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ch_ids_a=[13,12,10,0,0,0,0,0]\n",
    "mask=np.ones_like(input_ch_ids_a)\n",
    "mask[np.array(input_ch_ids_a)==0]=0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
    "\n",
    "        \n",
    "#         return q,k,v\n",
    "        mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output)) # n个head view 到一起后再线性变换到原始维度。\n",
    "        output = self.layer_norm(output + residual) # 加殘差。\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch=4\n",
    "len_sen=5\n",
    "dim=10\n",
    "mask=torch.ByteTensor([0,0,0,1,1])\n",
    "mask=mask.repeat(batch,1,1)\n",
    "n_head=7\n",
    "\n",
    "q=torch.rand(batch,len_sen,dim)\n",
    "mhattn=MultiHeadAttention(n_head,10,10,10)\n",
    "o,a=mhattn(q,q,q,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask=torch.Tensor([[1,1,0],[1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.reshape(mask.size(0),1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 800, 120])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_logits(inputs, mask):\n",
    "    mask = mask.type(torch.float32)\n",
    "    return inputs + (-1e30) * (1 - mask)\n",
    "\n",
    "\n",
    "class CQAttention(nn.Module):\n",
    "    def __init__(self,D,Lc,Lq,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=dropout\n",
    "        self.Lc=Lc\n",
    "        self.Lq=Lq        \n",
    "        w4C = torch.empty(D, 1)\n",
    "        w4Q = torch.empty(D, 1)\n",
    "        w4mlu = torch.empty(1, 1, D)\n",
    "        nn.init.xavier_uniform_(w4C)\n",
    "        nn.init.xavier_uniform_(w4Q)\n",
    "        nn.init.xavier_uniform_(w4mlu)\n",
    "        self.w4C = nn.Parameter(w4C)\n",
    "        self.w4Q = nn.Parameter(w4Q)\n",
    "        self.w4mlu = nn.Parameter(w4mlu)\n",
    "        \n",
    "        bias = torch.empty(1)\n",
    "        nn.init.constant_(bias, 0)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, C, Q, Cmask, Qmask):\n",
    "        C = C.transpose(1, 2)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        batch_size_c = C.size()[0]\n",
    "        S = self.trilinear_for_attention(C, Q)\n",
    "        Cmask = Cmask.view(batch_size_c, self.Lc, 1)\n",
    "        Qmask = Qmask.view(batch_size_c, 1, self.Lq)\n",
    "        S1 = F.softmax(mask_logits(S, Qmask), dim=2)\n",
    "        S2 = F.softmax(mask_logits(S, Cmask), dim=1)\n",
    "        A = torch.bmm(S1, Q)\n",
    "        B = torch.bmm(torch.bmm(S1, S2.transpose(1, 2)), C)\n",
    "        out = torch.cat([C, A, torch.mul(C, A), torch.mul(C, B)], dim=2)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "    def trilinear_for_attention(self, C, Q):\n",
    "        C = F.dropout(C, p=self.dropout, training=self.training)\n",
    "        Q = F.dropout(Q, p=self.dropout, training=self.training)\n",
    "#         print(C.shape,Q.shape)\n",
    "        subres0 = torch.matmul(C, self.w4C).expand([-1, -1,  self.Lq])\n",
    "        subres1 = torch.matmul(Q, self.w4Q).transpose(1, 2).expand([-1,  self.Lc, -1])\n",
    "        subres2 = torch.matmul(C * self.w4mlu, Q.transpose(1,2))\n",
    "#         print(subres0.shape,subres1.shape,subres2.shape)\n",
    "        res = subres0 + subres1 + subres2\n",
    "        res += self.bias\n",
    "        return res\n",
    "\n",
    "\n",
    "D=200\n",
    "L_q=120\n",
    "cqattn=CQAttention(D,L_q,L_q,0.1)\n",
    "\n",
    "C=torch.rand(128,D,L_q)\n",
    "Cmask=torch.Tensor([1]*L_q)\n",
    "Cmask=Cmask.repeat(128,1)\n",
    "\n",
    "Q=torch.rand(128,D,L_q)\n",
    "Qmask=torch.Tensor([1]*L_q)\n",
    "Qmask=Qmask.repeat(128,1)\n",
    "\n",
    "cqattn(C, Q, Cmask, Qmask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 200, 120])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 120])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
    "\n",
    "        \n",
    "#         return q,k,v\n",
    "        mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output)) # n个head view 到一起后再线性变换到原始维度。\n",
    "        output = self.layer_norm(output + residual) # 加殘差。\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "batch=4\n",
    "len_sen=5\n",
    "dim=10\n",
    "mask=torch.ByteTensor([0,0,0,1,1])\n",
    "mask=mask.repeat(batch,1,1)\n",
    "n_head=7\n",
    "\n",
    "q=torch.rand(batch,len_sen,dim)\n",
    "mhattn=MultiHeadAttention(n_head,10,10,10)\n",
    "o,a=mhattn(q,q,q,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
